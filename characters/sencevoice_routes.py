from collections import defaultdict
import json
from modelscope.utils.constant import Tasks
from modelscope.pipelines import pipeline
import os
from urllib.parse import parse_qs
import soundfile as sf
import numpy as np
from funasr import AutoModel
from pydantic import BaseModel, Field
from pydantic_settings import BaseSettings
from starlette.status import HTTP_422_UNPROCESSABLE_ENTITY
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.exceptions import RequestValidationError
from fastapi import APIRouter, FastAPI, WebSocket, WebSocketDisconnect, Request, HTTPException
import logging
# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


class Config(BaseSettings):
    sv_thr: float = Field(0.29, description="Speaker verification threshold")
    chunk_size_ms: int = Field(100, description="Chunk size in milliseconds")
    sample_rate: int = Field(16000, description="Sample rate in Hz")
    bit_depth: int = Field(16, description="Bit depth")
    channels: int = Field(1, description="Number of audio channels")


config = Config()

emo_dict = {
    "<|HAPPY|>": "ğŸ˜Š",
    "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡",
    "<|NEUTRAL|>": "",
    "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢",
    "<|SURPRISED|>": "ğŸ˜®",
}

event_dict = {
    "<|BGM|>": "ğŸ¼",
    "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘",
    "<|Laughter|>": "ğŸ˜€",
    "<|Cry|>": "ğŸ˜­",
    "<|Sneeze|>": "ğŸ¤§",
    "<|Breath|>": "",
    "<|Cough|>": "ğŸ¤§",
}

emoji_dict = {
    "<|nospeech|><|Event_UNK|>": "â“",
    "<|zh|>": "",
    "<|en|>": "",
    "<|yue|>": "",
    "<|ja|>": "",
    "<|ko|>": "",
    "<|nospeech|>": "",
    "<|HAPPY|>": "ğŸ˜Š",
    "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡",
    "<|NEUTRAL|>": "",
    "<|BGM|>": "ğŸ¼",
    "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘",
    "<|Laughter|>": "ğŸ˜€",
    "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢",
    "<|SURPRISED|>": "ğŸ˜®",
    "<|Cry|>": "ğŸ˜­",
    "<|EMO_UNKNOWN|>": "",
    "<|Sneeze|>": "ğŸ¤§",
    "<|Breath|>": "",
    "<|Cough|>": "ğŸ˜·",
    "<|Sing|>": "",
    "<|Speech_Noise|>": "",
    "<|withitn|>": "",
    "<|woitn|>": "",
    "<|GBG|>": "",
    "<|Event_UNK|>": "",
}

lang_dict = {
    "<|zh|>": "<|lang|>",
    "<|en|>": "<|lang|>",
    "<|yue|>": "<|lang|>",
    "<|ja|>": "<|lang|>",
    "<|ko|>": "<|lang|>",
    "<|nospeech|>": "<|lang|>",
}

emo_set = {"ğŸ˜Š", "ğŸ˜”", "ğŸ˜¡", "ğŸ˜°", "ğŸ¤¢", "ğŸ˜®"}
event_set = {"ğŸ¼", "ğŸ‘", "ğŸ˜€", "ğŸ˜­", "ğŸ¤§", "ğŸ˜·", }


def format_str(s):
    for sptk in emoji_dict:
        s = s.replace(sptk, emoji_dict[sptk])
    return s


def format_str_v2(s):
    sptk_dict = {}
    for sptk in emoji_dict:
        sptk_dict[sptk] = s.count(sptk)
        s = s.replace(sptk, "")
    emo = "<|NEUTRAL|>"
    for e in emo_dict:
        if sptk_dict[e] > sptk_dict[emo]:
            emo = e
    for e in event_dict:
        if sptk_dict[e] > 0:
            s = event_dict[e] + s
    s = s + emo_dict[emo]

    for emoji in emo_set.union(event_set):
        s = s.replace(" " + emoji, emoji)
        s = s.replace(emoji + " ", emoji)
    return s.strip()


def format_str_v3(s):
    def get_emo(s):
        return s[-1] if s[-1] in emo_set else None

    def get_event(s):
        return s[0] if s[0] in event_set else None

    s = s.replace("<|nospeech|><|Event_UNK|>", "â“")
    for lang in lang_dict:
        s = s.replace(lang, "<|lang|>")
    s_list = [format_str_v2(s_i).strip(" ") for s_i in s.split("<|lang|>")]
    new_s = " " + s_list[0]
    cur_ent_event = get_event(new_s)
    for i in range(1, len(s_list)):
        if len(s_list[i]) == 0:
            continue
        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:
            s_list[i] = s_list[i][1:]
        # else:
        cur_ent_event = get_event(s_list[i])
        if get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):
            new_s = new_s[:-1]
        new_s += s_list[i].strip().lstrip()
    new_s = new_s.replace("The.", " ")
    return new_s.strip()


sv_pipeline = pipeline(
    task='speaker-verification',
    model='iic/speech_campplus_sv_zh_en_16k-common_advanced',
    model_revision='v1.0.0'
)

asr_pipeline = pipeline(
    task=Tasks.auto_speech_recognition,
    model='iic/SenseVoiceSmall',
    model_revision="master",
    device="cuda:0",
)


model = AutoModel(
    model="fsmn-vad",
    model_revision="v2.0.4",
    disable_pbar=True,
    max_end_silence_time=200,
    speech_noise_thres=0.8
)

reg_spks_files = [
    "speaker/speaker1_a_cn_16k.wav"
]


def reg_spk_init(files):
    reg_spk = {}
    for f in files:
        data, sr = sf.read(f, dtype="float32")
        k, _ = os.path.splitext(os.path.basename(f))
        reg_spk[k] = {
            "data": data,
            "sr":   sr,
        }
    return reg_spk


reg_spks = reg_spk_init(reg_spks_files)


def process_vad_audio(audio, sv=True, lang="auto"):
    logger.debug(f"[process_vad_audio] process audio(length: {len(audio)})")
    if not sv:
        return asr_pipeline(audio, language=lang.strip())

    hit = False
    for k, v in reg_spks.items():
        res_sv = sv_pipeline([audio, v["data"]], thr=config.sv_thr)
        logger.debug(f"[speaker check] {k}: {res_sv}")
        if res_sv["score"] >= config.sv_thr:
            hit = True

    return asr_pipeline(audio, language=lang.strip()) if hit else None


router = APIRouter()
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    # Change to domains if you deploy this to production
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Define the response model


class TranscriptionResponse(BaseModel):
    code: int
    msg: str
    data: str


class Config(BaseSettings):
    sv_thr: float = Field(0.29, description="è¯´è¯äººéªŒè¯é˜ˆå€¼")
    chunk_size_ms: int = Field(100, description="å—å¤§å°ï¼ˆæ¯«ç§’ï¼‰")
    sample_rate: int = Field(16000, description="é‡‡æ ·ç‡ï¼ˆHzï¼‰")
    bit_depth: int = Field(16, description="ä½æ·±")
    channels: int = Field(1, description="éŸ³é¢‘é€šé“æ•°")


config = Config()


# å…¨å±€å­—å…¸ç”¨äºç®¡ç†æ¯ä¸ª clientMsgId çš„è½¬å½•ä¸Šä¸‹æ–‡
transcription_contexts = defaultdict(lambda: {
    "total_transcription": "",  # å­˜å‚¨ç´¯ç§¯çš„è½¬å½•æ–‡æœ¬
    "audio_chunks": [],
    "audio_vad": np.array([]),
    "cache": {},
    "audio_url": None,
    "last_vad_beg": -1,
    "last_vad_end": -1,
    "offset": 0
})


@app.exception_handler(Exception)
async def custom_exception_handler(request: Request, exc: Exception):
    logger.error("Exception occurred", exc_info=True)
    if isinstance(exc, HTTPException):
        status_code = exc.status_code
        message = exc.detail
        data = ""
    elif isinstance(exc, RequestValidationError):
        status_code = HTTP_422_UNPROCESSABLE_ENTITY
        message = "Validation error: " + str(exc.errors())
        data = ""
    else:
        status_code = 500
        message = "Internal server error: " + str(exc)
        data = ""

    return JSONResponse(
        status_code=status_code,
        content=TranscriptionResponse(
            code=status_code,
            msg=message,
            data=data
        ).model_dump()
    )




@router.websocket("/sencevoice/transcribe")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()

    # è§£ææŸ¥è¯¢å‚æ•°
    query_params = parse_qs(websocket.scope['query_string'].decode())
    sv = query_params.get('sv', ['false'])[0].lower() in [
        'true', '1', 't', 'y', 'yes']
    lang = query_params.get('lang', ['auto'])[0].lower()
    client_msg_id = query_params.get('clientMsgId', [None])[0]

    if not client_msg_id:
        logger.warning("æœªæä¾› clientMsgIdï¼Œå…³é—­è¿æ¥")
        await websocket.close(code=1008, reason="æœªæä¾› clientMsgId")
        return

    logger.info(f"å·²è¿æ¥ï¼ŒclientMsgId: {client_msg_id}")

    # åˆå§‹åŒ–ä¸Šä¸‹æ–‡
    context = transcription_contexts[client_msg_id]

    try:
        while True:
            message = await websocket.receive_text()
            logger.debug(f"æ”¶åˆ°æ¶ˆæ¯: {message}")
            data = json.loads(message)

            msg_type = data.get('type')
            received_client_msg_id = data.get('clientMsgId')

            # if received_client_msg_id != client_msg_id:
            #     logger.warning(
            #         f"clientMsgId ä¸åŒ¹é…: {received_client_msg_id} != {client_msg_id}")
            #     # å‘é€é”™è¯¯ä¿¡æ¯ç»™å‰ç«¯
            #     await websocket.send_json({
            #         "clientMsgId": client_msg_id,
            #         "code": 400,
            #         "msg": "clientMsgId ä¸åŒ¹é…"
            #     })
            #     continue

            if msg_type == 'audio':
                # å¤„ç†éŸ³é¢‘æ•°æ®
                audio_blob = data.get('audioBlob')  # é¢„æœŸä¸ºæ•´æ•°åˆ—è¡¨
                if not audio_blob:
                    logger.warning("æ¶ˆæ¯ä¸­æœªæ‰¾åˆ° audioBlob")
                    continue

                # å°† audioBlob åˆ—è¡¨è½¬æ¢ä¸ºå­—èŠ‚
                audio_bytes = bytes(audio_blob)

                # å°†å­—èŠ‚è½¬æ¢ä¸º numpy æ•°ç»„
                audio_np = np.frombuffer(
                    audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0

                # è¿½åŠ åˆ°éŸ³é¢‘ç¼“å†²åŒº
                context["audio_chunks"].append(audio_np)

                # åˆå¹¶æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
                combined_audio = np.concatenate(context["audio_chunks"])

                # è®¾å®šæ¯æ¬¡å¤„ç†çš„å—å¤§å°
                chunk_size = int(config.chunk_size_ms * config.sample_rate *
                                 config.channels * (config.bit_depth // 8) / 1000)

                # åˆ†å—å¤„ç†
                while len(combined_audio) >= chunk_size:
                    chunk = combined_audio[:chunk_size]
                    combined_audio = combined_audio[chunk_size:]
                    context["audio_vad"] = np.append(
                        context["audio_vad"], chunk)

                    # VAD æ¨ç†
                    res = model.generate(
                        input=chunk,
                        cache=context["cache"],
                        is_final=False,
                        chunk_size=config.chunk_size_ms
                    )
                    logger.debug(f"VAD æ¨ç†ç»“æœ: {res}")

                    if len(res[0]["value"]):
                        vad_segments = res[0]["value"]
                        for segment in vad_segments:
                            if segment[0] > -1:  # è¯­éŸ³å¼€å§‹
                                context["last_vad_beg"] = segment[0]
                            if segment[1] > -1:  # è¯­éŸ³ç»“æŸ
                                context["last_vad_end"] = segment[1]

                            if context["last_vad_beg"] > -1 and context["last_vad_end"] > -1:
                                logger.debug(
                                    f"VAD è¯­éŸ³æ®µ: {[context['last_vad_beg'], context['last_vad_end']]}")
                                context["last_vad_beg"] -= context["offset"]
                                context["last_vad_end"] -= context["offset"]
                                context["offset"] += context["last_vad_end"]
                                beg = int(context["last_vad_beg"]
                                          * config.sample_rate / 1000)
                                end = int(context["last_vad_end"]
                                          * config.sample_rate / 1000)

                                # ç¡®ä¿ç´¢å¼•åœ¨èŒƒå›´å†…
                                beg = max(0, beg)
                                end = min(len(context["audio_vad"]), end)

                                result = process_vad_audio(
                                    context["audio_vad"][beg:end], sv, lang
                                )
                                logger.debug(
                                    f"[process_vad_audio] ç»“æœ: {result}")
                                context["audio_vad"] = context["audio_vad"][end:]
                                context["last_vad_beg"] = context["last_vad_end"] = -1

                                if result is not None:
                                    current_transcription = format_str_v3(result[0]['text']).strip()
                                    # æ‰¾åˆ°æ–°å¢çš„éƒ¨åˆ†
                                    previous_transcription = context["total_transcription"]
                                    new_part = current_transcription[len(previous_transcription):]
                                    if new_part:
                                        # æ›´æ–°ç´¯ç§¯çš„è½¬å½•æ–‡æœ¬
                                        context["total_transcription"] = current_transcription

                                        # å‘é€æ–°å¢çš„éƒ¨åˆ†
                                        response = {
                                            "clientMsgId": client_msg_id,
                                            "transcriptionText": new_part.strip()
                                        }
                                        await websocket.send_json(response)

                # æ›´æ–°éŸ³é¢‘ç¼“å†²åŒº
                context["audio_chunks"] = [combined_audio]

            elif msg_type == 'transcription':
                # å¤„ç†è½¬å½•æ–‡æœ¬ç¡®è®¤
                transcription_text = data.get('content', '')
                context["transcription"] += transcription_text + " "
                # å‘é€ç¡®è®¤ä¿¡æ¯
                await websocket.send_json({
                    "clientMsgId": client_msg_id,
                    "code": 0,
                    "msg": "transcription_received"
                })

            elif msg_type == 'audioUrl':
                # å¤„ç†éŸ³é¢‘ URL
                audio_url = data.get('audioUrl', '')
                context["audio_url"] = audio_url
                # å‘é€ç¡®è®¤ä¿¡æ¯
                await websocket.send_json({
                    "clientMsgId": client_msg_id,
                    "code": 0,
                    "msg": "audioUrl_received"
                })

            elif msg_type == 'end':
                # å¤„ç† 'end' æ¶ˆæ¯
                logger.info(f"æ”¶åˆ° 'end' æ¶ˆæ¯ï¼ŒclientMsgId: {client_msg_id}")
                # å¤„ç†å‰©ä½™çš„éŸ³é¢‘æ•°æ®
                if len(context["audio_vad"]) > 0:
                    result = process_vad_audio(
                        context["audio_vad"], sv, lang
                    )
                    logger.debug(f"[process_vad_audio] æœ€ç»ˆç»“æœ: {result}")
                    if result is not None:
                        # æ›´æ–°è½¬å½•ä¸Šä¸‹æ–‡
                        context["total_transcription"] += format_str_v3(
                            result[0]['text']) + " "

                        # å‘é€æœ€ç»ˆçš„è½¬å½•ç»“æœ
                        response = {
                            "type": "end",
                            "clientMsgId": client_msg_id,
                            "transcriptionText": format_str_v3(result[0]['text'])
                        }
                        await websocket.send_json(response)

                # å¯é€‰ï¼šå‘é€æœ€ç»ˆç¡®è®¤æ¶ˆæ¯
                await websocket.send_json({
                    "clientMsgId": client_msg_id,
                    "code": 0,
                    "msg": "Final transcription completed"
                })

                # å…³é—­ WebSocket è¿æ¥
                await websocket.close()
                # æ¸…ç†ä¸Šä¸‹æ–‡
                if client_msg_id in transcription_contexts:
                    del transcription_contexts[client_msg_id]
                break  # é€€å‡ºå¾ªç¯

            else:
                logger.warning(f"æœªçŸ¥çš„æ¶ˆæ¯ç±»å‹: {msg_type}")
                # å‘é€é”™è¯¯ä¿¡æ¯
                await websocket.send_json({
                    "clientMsgId": client_msg_id,
                    "code": 400,
                    "msg": f"æœªçŸ¥çš„æ¶ˆæ¯ç±»å‹: {msg_type}"
                })

    except WebSocketDisconnect:
        logger.info(f"WebSocket æ–­å¼€è¿æ¥ï¼ŒclientMsgId: {client_msg_id}")
        # æ¸…ç†ä¸Šä¸‹æ–‡
        if client_msg_id in transcription_contexts:
            del transcription_contexts[client_msg_id]
    except Exception as e:
        logger.error(f"å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        await websocket.close()
        if client_msg_id in transcription_contexts:
            del transcription_contexts[client_msg_id]
